# Complete Deployment & Integration Guide

**Ultimate Goal:** User searches "Urdu Phishing Detector" ‚Üí Opens website ‚Üí Enters message ‚Üí Scans for scam ‚Üí Gets result

**Date:** October 22, 2025  
**Team:** Zaryab Khattak (Frontend) + Sibghat Ullah (Backend + AI Model)

---

## üìã Table of Contents

1. [System Architecture Overview](#system-architecture-overview)
2. [Backend Integration (What Sibghat Needs to Do)](#backend-integration-what-sibghat-needs-to-do)
3. [Frontend Configuration (What Zaryab Needs to Do)](#frontend-configuration-what-zaryab-needs-to-do)
4. [ML Model Hosting](#ml-model-hosting)
5. [Backend Hosting](#backend-hosting)
6. [Frontend Hosting](#frontend-hosting)
7. [Complete Deployment Flow](#complete-deployment-flow)
8. [Testing the Full System](#testing-the-full-system)
9. [Troubleshooting](#troubleshooting)

---

## üèóÔ∏è System Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         USER'S BROWSER                           ‚îÇ
‚îÇ  Searches: "Urdu Phishing Detector"                             ‚îÇ
‚îÇ  Visits: https://urdu-phishing-detector.vercel.app              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FRONTEND (React + Vite)                       ‚îÇ
‚îÇ  Hosted on: Vercel / Netlify / GitHub Pages                     ‚îÇ
‚îÇ  - User enters Urdu message                                     ‚îÇ
‚îÇ  - Clicks "Scan for Scam"                                       ‚îÇ
‚îÇ  - Sends message to backend API                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚îÇ HTTP POST Request
                         ‚îÇ { "message": "Your Urdu text..." }
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    BACKEND (FastAPI/Flask)                       ‚îÇ
‚îÇ  Hosted on: Render / Railway / PythonAnywhere                   ‚îÇ
‚îÇ  - Receives message from frontend                               ‚îÇ
‚îÇ  - Preprocesses the text                                        ‚îÇ
‚îÇ  - Sends to ML Model for prediction                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚îÇ Prediction Request
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ML MODEL (Trained Model)                      ‚îÇ
‚îÇ  Options:                                                        ‚îÇ
‚îÇ  1. Embedded in Backend (model.pkl loaded in memory)            ‚îÇ
‚îÇ  2. Separate API (Hugging Face / AWS SageMaker)                 ‚îÇ
‚îÇ  - Analyzes Urdu text                                           ‚îÇ
‚îÇ  - Returns: { "label": "SAFE/PHISHING", "confidence": 0.92 }   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚îÇ Response
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         BACKEND                                  ‚îÇ
‚îÇ  - Receives ML prediction                                       ‚îÇ
‚îÇ  - Formats response                                             ‚îÇ
‚îÇ  - Sends back to frontend                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚îÇ HTTP Response
                         ‚îÇ { "label": "SAFE", "confidence": 0.92 }
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         FRONTEND                                 ‚îÇ
‚îÇ  - Receives result                                              ‚îÇ
‚îÇ  - Displays to user:                                            ‚îÇ
‚îÇ    ‚úÖ "This message appears SAFE (92% confidence)"              ‚îÇ
‚îÇ    or                                                           ‚îÇ
‚îÇ    ‚ö†Ô∏è "WARNING: Potential phishing detected (87% confidence)"   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß Backend Integration (What Sibghat Needs to Do)

### Step 1: Backend Code Structure

Sibghat needs to create this structure:

```
urdu-phishing-backend/
‚îú‚îÄ‚îÄ main.py                 # FastAPI/Flask main file
‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îú‚îÄ‚îÄ model.pkl          # Trained ML model
‚îÇ   ‚îú‚îÄ‚îÄ vectorizer.pkl     # Text vectorizer
‚îÇ   ‚îî‚îÄ‚îÄ preprocessor.py    # Text preprocessing
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ .env                   # Environment variables
‚îî‚îÄ‚îÄ README.md
```

### Step 2: Backend API Code (FastAPI Example)

**File: `main.py`**

```python
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import pickle
import os

app = FastAPI(title="Urdu Phishing Detector API")

# CRITICAL: CORS Configuration
# Allow your frontend URL to access this API
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",  # Local development
        "https://urdu-phishing-detector.vercel.app",  # Production frontend
        "https://your-frontend-url.netlify.app",  # Alternative
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Load ML Model (Sibghat's trained model)
MODEL_PATH = os.getenv("MODEL_PATH", "model/model.pkl")
VECTORIZER_PATH = os.getenv("VECTORIZER_PATH", "model/vectorizer.pkl")

try:
    with open(MODEL_PATH, 'rb') as f:
        model = pickle.load(f)
    with open(VECTORIZER_PATH, 'rb') as f:
        vectorizer = pickle.load(f)
    print("‚úÖ Model loaded successfully")
except Exception as e:
    print(f"‚ùå Error loading model: {e}")
    model = None
    vectorizer = None

# Request/Response Models
class PredictionRequest(BaseModel):
    message: str

class PredictionResponse(BaseModel):
    label: str  # "SAFE", "SUSPICIOUS", or "PHISHING"
    confidence: float
    message: str

# Health Check Endpoint
@app.get("/")
def read_root():
    return {
        "status": "online",
        "service": "Urdu Phishing Detector API",
        "version": "1.0.0"
    }

# Main Prediction Endpoint
@app.post("/api/analyze", response_model=PredictionResponse)
def analyze_message(request: PredictionRequest):
    try:
        if not model or not vectorizer:
            raise HTTPException(
                status_code=503, 
                detail="Model not loaded. Please contact administrator."
            )
        
        # Preprocess message
        message = request.message.strip()
        
        if not message:
            raise HTTPException(
                status_code=400, 
                detail="Message cannot be empty"
            )
        
        # Vectorize text
        X = vectorizer.transform([message])
        
        # Get prediction
        prediction = model.predict(X)[0]
        
        # Get confidence (probability)
        if hasattr(model, 'predict_proba'):
            probabilities = model.predict_proba(X)[0]
            confidence = float(max(probabilities))
        else:
            confidence = 0.85  # Default confidence
        
        # Map prediction to label
        label_map = {
            0: "SAFE",
            1: "SUSPICIOUS",
            2: "PHISHING"
        }
        
        label = label_map.get(prediction, "UNKNOWN")
        
        return {
            "label": label,
            "confidence": round(confidence, 2),
            "message": f"Analysis complete. Message classified as {label}."
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error processing request: {str(e)}"
        )

# Run with: uvicorn main:app --reload
if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
```

**File: `requirements.txt`**

```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.4.2
scikit-learn==1.3.2
numpy==1.26.2
pandas==2.1.3
python-dotenv==1.0.0
```

### Step 3: Test Backend Locally

```bash
# Install dependencies
pip install -r requirements.txt

# Run server
python main.py

# Test endpoint
curl -X POST http://localhost:8000/api/analyze \
  -H "Content-Type: application/json" \
  -d '{"message": "Aap ne prize jeeta hai! Link par click karen"}'
```

**Expected Response:**
```json
{
  "label": "PHISHING",
  "confidence": 0.89,
  "message": "Analysis complete. Message classified as PHISHING."
}
```

---

## ‚öôÔ∏è Frontend Configuration (What Zaryab Needs to Do)

### Step 1: Create Environment Configuration

**File: `.env` (for local development)**

```env
# Local Development
VITE_API_URL=http://localhost:8000
VITE_ENABLE_MOCK_API=false
```

**File: `.env.production` (for production)**

```env
# Production (update after Sibghat deploys backend)
VITE_API_URL=https://urdu-phishing-api.onrender.com
VITE_ENABLE_MOCK_API=false
```

### Step 2: Update API Service

**File: `src/services/apiService.js`** (Already created, just verify)

```javascript
import { APP_CONFIG } from '../constants/config';

const API_BASE_URL = import.meta.env.VITE_API_URL || 'http://localhost:8000';
const USE_MOCK_API = import.meta.env.VITE_ENABLE_MOCK_API === 'true';

export const analyzeMessage = async (message) => {
  if (USE_MOCK_API) {
    // Mock response for testing
    await new Promise(resolve => setTimeout(resolve, 2000));
    return {
      label: Math.random() > 0.5 ? 'SAFE' : 'PHISHING',
      confidence: 0.85 + Math.random() * 0.15,
      message: 'Mock analysis complete'
    };
  }

  // Real API call
  try {
    const response = await fetch(`${API_BASE_URL}/api/analyze`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ message }),
    });

    if (!response.ok) {
      throw new Error(`API Error: ${response.status}`);
    }

    const data = await response.json();
    return data;
  } catch (error) {
    console.error('API Error:', error);
    throw new Error('Failed to analyze message. Please try again.');
  }
};
```

### Step 3: Test Integration Locally

```powershell
# 1. Make sure backend is running (Sibghat)
# Backend should be at: http://localhost:8000

# 2. Update .env
echo "VITE_API_URL=http://localhost:8000" > .env
echo "VITE_ENABLE_MOCK_API=false" >> .env

# 3. Start frontend
npm run dev

# 4. Open browser: http://localhost:5173
# 5. Test by entering a message and clicking scan
```

---

## ü§ñ ML Model Hosting

### Option 1: Embed in Backend (RECOMMENDED - Easiest)

**Pros:**
- ‚úÖ Simple setup
- ‚úÖ No additional hosting needed
- ‚úÖ Fast response times
- ‚úÖ No extra costs

**Cons:**
- ‚ùå Increases backend memory usage
- ‚ùå Harder to update model independently

**How:**
1. Sibghat saves trained model as `model.pkl`
2. Includes it in backend repository
3. Loads it in `main.py` (as shown above)

```python
# In main.py
with open('model/model.pkl', 'rb') as f:
    model = pickle.load(f)
```

---

### Option 2: Separate Model API (Advanced)

**Use Hugging Face Inference API:**

1. **Upload model to Hugging Face:**
   ```bash
   huggingface-cli login
   python upload_model.py
   ```

2. **Call from backend:**
   ```python
   import requests
   
   API_URL = "https://api-inference.huggingface.co/models/your-model"
   headers = {"Authorization": f"Bearer {API_TOKEN}"}
   
   def query(payload):
       response = requests.post(API_URL, headers=headers, json=payload)
       return response.json()
   
   result = query({"inputs": "Urdu text here"})
   ```

**Only use this if:**
- Model is very large (>500MB)
- You want to update model independently
- You have budget for cloud hosting

---

## üöÄ Backend Hosting

### Option 1: Render.com (RECOMMENDED - Free Tier)

**Steps:**

1. **Create account:** https://render.com

2. **Create new Web Service:**
   - Click "New +" ‚Üí "Web Service"
   - Connect GitHub account
   - Select `urdu-phishing-backend` repository

3. **Configure:**
   ```
   Name: urdu-phishing-api
   Environment: Python 3
   Build Command: pip install -r requirements.txt
   Start Command: uvicorn main:app --host 0.0.0.0 --port $PORT
   ```

4. **Environment Variables:**
   ```
   PORT=10000
   MODEL_PATH=model/model.pkl
   VECTORIZER_PATH=model/vectorizer.pkl
   ```

5. **Deploy:**
   - Click "Create Web Service"
   - Wait for deployment (~5 minutes)
   - Get URL: `https://urdu-phishing-api.onrender.com`

6. **Test:**
   ```bash
   curl https://urdu-phishing-api.onrender.com/
   # Should return: {"status": "online"}
   ```

**Free Tier Limits:**
- ‚úÖ 750 hours/month (enough for 24/7)
- ‚úÖ Auto-sleep after 15 min inactivity
- ‚úÖ 512MB RAM
- ‚ö†Ô∏è Spins down when idle (first request takes ~30s)

---

### Option 2: Railway.app

Similar to Render, also has free tier.

1. Go to: https://railway.app
2. Connect GitHub
3. Deploy from repository
4. Set environment variables
5. Get deployment URL

---

### Option 3: PythonAnywhere (Good for Python)

1. Go to: https://www.pythonanywhere.com
2. Create free account
3. Upload code via Git
4. Configure WSGI
5. Get URL: `https://yourusername.pythonanywhere.com`

---

## üåê Frontend Hosting

### Option 1: Vercel (RECOMMENDED - Best for React/Vite)

**Steps:**

1. **Create account:** https://vercel.com

2. **Import Project:**
   - Click "Add New" ‚Üí "Project"
   - Import from GitHub
   - Select `Urdu-Phishing-Detector-` repository

3. **Configure:**
   ```
   Framework Preset: Vite
   Build Command: npm run build
   Output Directory: dist
   Install Command: npm install
   ```

4. **Environment Variables:**
   ```
   VITE_API_URL=https://urdu-phishing-api.onrender.com
   VITE_ENABLE_MOCK_API=false
   ```

5. **Deploy:**
   - Click "Deploy"
   - Wait 2-3 minutes
   - Get URL: `https://urdu-phishing-detector.vercel.app`

6. **Custom Domain (Optional):**
   - Buy domain: `urduphishingdetector.com`
   - Add in Vercel settings
   - Update DNS records

**Free Tier:**
- ‚úÖ Unlimited deployments
- ‚úÖ Automatic HTTPS
- ‚úÖ Global CDN
- ‚úÖ Auto-deploys on Git push

---

### Option 2: Netlify

1. Go to: https://www.netlify.com
2. Drag & drop `dist/` folder
3. Or connect GitHub for auto-deploy

---

### Option 3: GitHub Pages (Free but limited)

```powershell
# Build
npm run build

# Deploy
npm install -g gh-pages
gh-pages -d dist
```

URL: `https://zaryabkhattak.github.io/Urdu-Phishing-Detector-`

---

## üîÑ Complete Deployment Flow

### Phase 1: Local Testing (DO THIS FIRST)

**Day 1-2: Sibghat's Tasks**
```bash
# 1. Create backend repository
# 2. Add model files (model.pkl, vectorizer.pkl)
# 3. Create main.py with API endpoint
# 4. Test locally:
python main.py
# Should run on http://localhost:8000

# 5. Test endpoint:
curl -X POST http://localhost:8000/api/analyze \
  -H "Content-Type: application/json" \
  -d '{"message": "Test message"}'
```

**Day 1-2: Zaryab's Tasks**
```powershell
# 1. Update .env file:
echo "VITE_API_URL=http://localhost:8000" > .env
echo "VITE_ENABLE_MOCK_API=false" >> .env

# 2. Start frontend:
npm run dev

# 3. Test integration:
# - Open http://localhost:5173
# - Enter message
# - Click scan
# - Verify you see real results from backend
```

**Success Criteria:**
- ‚úÖ Backend returns predictions
- ‚úÖ Frontend displays results
- ‚úÖ No CORS errors
- ‚úÖ Confidence scores display correctly

---

### Phase 2: Deploy Backend

**Day 3: Sibghat Deploys Backend**

```bash
# 1. Push backend to GitHub
git init
git add .
git commit -m "Initial backend with ML model"
git remote add origin https://github.com/sibghat/urdu-phishing-backend.git
git push -u origin main

# 2. Deploy to Render.com (follow steps above)

# 3. Test production API:
curl https://urdu-phishing-api.onrender.com/

# 4. Share URL with Zaryab:
"Backend is live at: https://urdu-phishing-api.onrender.com"
```

---

### Phase 3: Update Frontend Configuration

**Day 3: Zaryab Updates Frontend**

```powershell
# 1. Update .env.production
echo "VITE_API_URL=https://urdu-phishing-api.onrender.com" > .env.production
echo "VITE_ENABLE_MOCK_API=false" >> .env.production

# 2. Test with production API locally:
npm run build
npm run preview
# Open http://localhost:4173 and test

# 3. If working, commit changes:
git add .env.production
git commit -m "Configure production API URL"
git push origin main
```

---

### Phase 4: Deploy Frontend

**Day 4: Zaryab Deploys Frontend**

```
1. Go to Vercel.com
2. Import GitHub repository
3. Add environment variable:
   VITE_API_URL=https://urdu-phishing-api.onrender.com
4. Deploy
5. Get URL: https://urdu-phishing-detector.vercel.app
6. Test end-to-end
```

---

### Phase 5: Final Testing

**Day 4-5: Both Test Together**

Test these scenarios:

1. **Happy Path:**
   - User enters safe message
   - Gets "SAFE" result with confidence

2. **Phishing Detection:**
   - User enters phishing message
   - Gets "PHISHING" warning

3. **Edge Cases:**
   - Empty message ‚Üí Error message
   - Very long message ‚Üí Still works
   - Special characters ‚Üí Handles correctly

4. **Performance:**
   - Response time < 5 seconds
   - No timeout errors
   - Works on mobile

5. **Error Handling:**
   - Backend down ‚Üí Friendly error message
   - Invalid input ‚Üí Clear validation

---

## üß™ Testing the Full System

### Test Checklist

**Frontend Tests:**
```powershell
# Local
npm run dev
# Visit: http://localhost:5173
# Test: Enter message ‚Üí Scan ‚Üí See result

# Production Preview
npm run build
npm run preview
# Visit: http://localhost:4173
# Test same flow

# Production
# Visit: https://urdu-phishing-detector.vercel.app
# Test same flow
```

**Backend Tests:**
```bash
# Health check
curl https://urdu-phishing-api.onrender.com/

# Analyze endpoint
curl -X POST https://urdu-phishing-api.onrender.com/api/analyze \
  -H "Content-Type: application/json" \
  -d '{"message": "Aap ne prize jeeta hai"}'
```

**Integration Tests:**
1. Open frontend in browser
2. Open browser DevTools (F12) ‚Üí Network tab
3. Enter message and click scan
4. Verify:
   - Request goes to correct API URL
   - Response status: 200
   - Response contains label and confidence
   - UI updates with result

---

## üîß Configuration Files Summary

### Sibghat's Backend Files

**`main.py`** - API endpoints (provided above)

**`requirements.txt`** - Python dependencies
```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.4.2
scikit-learn==1.3.2
numpy==1.26.2
```

**`.env`** - Local environment variables
```env
PORT=8000
MODEL_PATH=model/model.pkl
VECTORIZER_PATH=model/vectorizer.pkl
```

**`render.yaml`** - Render configuration (optional)
```yaml
services:
  - type: web
    name: urdu-phishing-api
    env: python
    buildCommand: pip install -r requirements.txt
    startCommand: uvicorn main:app --host 0.0.0.0 --port $PORT
```

---

### Zaryab's Frontend Files

**`.env`** - Local development
```env
VITE_API_URL=http://localhost:8000
VITE_ENABLE_MOCK_API=false
```

**`.env.production`** - Production
```env
VITE_API_URL=https://urdu-phishing-api.onrender.com
VITE_ENABLE_MOCK_API=false
```

**`vercel.json`** - Vercel configuration (optional)
```json
{
  "buildCommand": "npm run build",
  "outputDirectory": "dist",
  "framework": "vite"
}
```

---

## üêõ Troubleshooting

### Problem: CORS Error

**Error:**
```
Access to fetch at 'https://api.com' from origin 'https://frontend.com' 
has been blocked by CORS policy
```

**Solution (Sibghat):**
```python
# In main.py, update allow_origins:
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://urdu-phishing-detector.vercel.app",  # Add your actual frontend URL
        "http://localhost:5173",
    ],
    allow_methods=["*"],
    allow_headers=["*"],
)
```

---

### Problem: Backend Returns 404

**Check:**
1. Correct endpoint: `/api/analyze` (not `/predict`)
2. Backend is running
3. URL is correct in `.env`

**Fix (Zaryab):**
```env
# Make sure no trailing slash
VITE_API_URL=https://urdu-phishing-api.onrender.com
# NOT: https://urdu-phishing-api.onrender.com/
```

---

### Problem: Model Not Loading

**Error:**
```
Error loading model: No such file or directory
```

**Fix (Sibghat):**
```bash
# Ensure model files are in repository
git add model/model.pkl
git add model/vectorizer.pkl
git commit -m "Add ML model files"
git push

# Check file size (GitHub has 100MB limit)
ls -lh model/
# If > 100MB, use Git LFS
git lfs install
git lfs track "*.pkl"
```

---

### Problem: Frontend Shows "Network Error"

**Possible Causes:**
1. Backend is down
2. Wrong API URL
3. Backend is sleeping (Render free tier)

**Solution:**
```javascript
// Add retry logic in apiService.js
const analyzeMessage = async (message, retries = 3) => {
  for (let i = 0; i < retries; i++) {
    try {
      const response = await fetch(`${API_BASE_URL}/api/analyze`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ message }),
      });
      
      if (response.ok) {
        return await response.json();
      }
    } catch (error) {
      if (i === retries - 1) throw error;
      await new Promise(r => setTimeout(r, 2000)); // Wait 2s before retry
    }
  }
};
```

---

### Problem: Vercel Deployment Fails

**Check:**
1. Build command is correct: `npm run build`
2. Output directory is: `dist`
3. Environment variables are set
4. Dependencies are in `package.json`

**Fix:**
```powershell
# Test build locally first
npm run build
# Should create dist/ folder

# If it works locally but fails on Vercel:
# Check Vercel build logs for specific error
```

---

## üìä Deployment Costs

### Free Tier (RECOMMENDED for Start)

| Service | Cost | Limits |
|---------|------|--------|
| **Render** (Backend) | $0/month | 750 hours, sleeps after 15min idle |
| **Vercel** (Frontend) | $0/month | Unlimited, 100GB bandwidth |
| **Total** | **$0/month** | Perfect for starting out |

### Paid Tier (When You Get Traffic)

| Service | Cost | Benefits |
|---------|------|----------|
| **Render** (Backend) | $7/month | No sleep, better performance |
| **Vercel Pro** (Frontend) | $20/month | Analytics, better support |
| **Domain** | $12/year | Custom domain like urduphishing.com |
| **Total** | **~$28/month** | Professional setup |

---

## üéØ Success Criteria

### ‚úÖ You Know You're Done When:

1. **User can search "Urdu Phishing Detector" and find your site** (SEO takes time)
2. **User enters message on your website**
3. **Clicks "Scan for Scam"**
4. **Sees result within 5 seconds**
5. **Result is accurate** (from real ML model, not mock)
6. **Works on mobile and desktop**
7. **No errors in browser console**
8. **Backend stays online 24/7**

---

## üìû Communication Template

### When Sibghat Finishes Backend

**Sibghat to Zaryab:**
```
‚úÖ Backend deployed!

API URL: https://urdu-phishing-api.onrender.com

Test it:
curl https://urdu-phishing-api.onrender.com/

Endpoints:
- GET  /           ‚Üí Health check
- POST /api/analyze ‚Üí Analyze message

Request format:
{
  "message": "Your Urdu text"
}

Response format:
{
  "label": "SAFE" | "PHISHING" | "SUSPICIOUS",
  "confidence": 0.92,
  "message": "Analysis complete..."
}

Ready for frontend integration!
```

### When Zaryab Needs to Update

**Zaryab to Sibghat:**
```
Need to update frontend config with your API URL.

Please confirm:
1. Final API URL: ___________
2. Endpoint path: /api/analyze (correct?)
3. CORS configured for: https://urdu-phishing-detector.vercel.app

Once confirmed, I'll deploy frontend.
```

---

## üöÄ Timeline

**Realistic Timeline:**

| Day | Tasks | Owner |
|-----|-------|-------|
| **Day 1** | Test backend locally | Sibghat |
| **Day 1** | Test frontend with local backend | Zaryab |
| **Day 2** | Deploy backend to Render | Sibghat |
| **Day 2** | Test production backend | Both |
| **Day 3** | Update frontend config | Zaryab |
| **Day 3** | Deploy frontend to Vercel | Zaryab |
| **Day 4** | End-to-end testing | Both |
| **Day 5** | Fix bugs, optimize | Both |
| **Day 6** | Final testing, documentation | Both |
| **Day 7** | Launch! üöÄ | Both |

**Total: 1 week from now to live website!**

---

## üìù Final Checklist

### Before Launch:

**Sibghat:**
- [ ] Backend code on GitHub
- [ ] Model files included
- [ ] CORS configured correctly
- [ ] Deployed to Render/Railway
- [ ] Health check endpoint works
- [ ] Analyze endpoint tested
- [ ] Shared API URL with Zaryab

**Zaryab:**
- [ ] Frontend code on GitHub
- [ ] `.env.production` configured
- [ ] Tested with production API
- [ ] Deployed to Vercel
- [ ] Environment variables set
- [ ] Custom domain added (optional)
- [ ] Mobile tested

**Both:**
- [ ] End-to-end test successful
- [ ] Error handling works
- [ ] Performance acceptable (<5s response)
- [ ] README updated
- [ ] Documentation complete

---

## üéä After Launch

### Monitor:

1. **Vercel Analytics** - See how many users visit
2. **Backend Logs** - Check for errors
3. **User Feedback** - Collect and improve

### Promote:

1. Share on LinkedIn
2. Add to portfolio
3. Submit to product directories
4. Share with Urdu-speaking communities

### Improve:

1. Add more features
2. Improve ML model accuracy
3. Add analytics
4. Optimize performance

---

**YOU'RE READY TO DEPLOY! üöÄ**

**Next Steps:**
1. Sibghat: Follow "Backend Integration" section
2. Test locally together
3. Deploy backend
4. Update frontend
5. Deploy frontend
6. LAUNCH!

Good luck! You've got this! üí™
